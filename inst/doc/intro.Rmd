---
title: "分类与聚类算法的实现与分析"
author: "康昊文 SA24231055"
date: "2024/12/08"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: vignette
    toc: true         # 开启目录
    toc_depth: 5      # 目录深度，5 表示包括五级标题
editor_options: 
  markdown: 
    wrap: 72
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
/* 目录样式 */
#TOC {
  position: fixed;
  left: 0px;
  top: 0px;
  width: 320px; /* 初始宽度 */
  height: 100%;
  overflow: auto;
  background-color: #ffffff;
  border-right: 3px solid #007bff;
  padding: 15px;
  box-shadow: 5px 0 10px rgba(0,0,0,0.1);
  font-family: 'Arial', sans-serif;
  font-size: 16px; /* 调节字体大小 */
  resize: horizontal; /* 允许水平方向调整宽度 */
  max-width: 400px; /* 设置最大宽度 */
  min-width: 200px; /* 设置最小宽度 */
  z-index: 1000;
}

/* 主内容区样式 */
.main-content {
  margin-left: calc(320px + 2px); /* 初始目录宽度 + 2px 的间距 */
}

body {
  margin-left: calc(320px + 2px); /* 适用于整个页面的内容 */
}

/* 居中标题样式 */
.center-title {
  text-align: center;
  font-size: 1.5em;
  font-weight: bold;
  margin-top: 20px;
  margin-bottom: 20px;
  color: black !important; /* 设置字体颜色为黑色 */
}

</style>

## R包概述

本R包旨在实现并分析多种经典的分类与聚类算法，提供高效且易用的工具，适用于数据科学与机器学习领域的教学与研究。主要功能包括以下几个方面：

### R包主要功能

1. **分类算法实现**：
   - **Softmax 多分类算法**：提供了R语言实现（`softmaxR`）和Rcpp（C++）实现（`softmaxCpp`），适用于多类别分类问题。
   - **决策树分类算法**：实现了R语言版本（`decisionTreeR`），支持基于信息增益的树构建，并限制树的最大深度以防止过拟合。
   - **朴素贝叶斯分类算法**：包括R语言实现（`naiveBayesR`）和Rcpp实现（`naiveBayesCpp`），适合高维数据的快速分类。

2. **聚类算法实现**：
   - **K-Means 算法**：提供了R语言版本（`kmeansR`）和Rcpp版本（`kmeansCpp`），用于将数据划分为预定数量的簇。
   - **K-Means++ 算法**：包括R语言实现（`kmeansppR`）和Rcpp实现（`kmeansppCpp`），通过改进的初始中心选择策略提高聚类效果和收敛速度。
   - **谱聚类算法**：实现了R语言版本（`spectralClusteringR`）和Rcpp版本（`spectralClusteringRcpp`），适用于处理具有复杂结构的数据。

3. **性能优化与比较**：
   - 利用`microbenchmark`包对R语言和Rcpp版本的各类算法进行性能基准测试，直观展现不同实现方式的计算效率差异。

4. **可视化工具**：
   - 包含多种绘图函数，如`plotSoftmaxLoss`、`plotSoftmaxAccuracy`、`plotDecisionTreeAccuracy`、`plotKMeansResult`等，用于直观展示算法的训练过程、性能指标及聚类结果。

5. **作业汇总**：
   - 本包汇总了《统计计算》课程的所有平时作业，详细内容可参见`inst/doc/homework.html`。这些作业涵盖了算法的理论推导、代码实现以及性能分析等，仅供参考。

### R包的结构介绍

本R包的目录结构设计合理，便于用户查找和使用各类资源。主要目录及文件说明如下：

- **`data/`**：
  - 存放数据集文件，包括：
    - `films_train.rda`：用于分类算法的训练数据集。
    - `films_test.rda`：用于分类算法的测试数据集。
    - `ori_data.rda`：用于聚类算法的数据集。

- **`R/`**：
  - 包含所有R语言脚本文件，实现了各类算法的R版本和辅助函数：
    - **分类算法相关**：
      - `softmaxR.R`、`decisionTreeR.R`、`naiveBayesR.R`
      - 绘图函数：`plotSoftmaxLoss.R`、`plotSoftmaxAccuracy.R`、`plotDecisionTreeAccuracy.R`
    - **聚类算法相关**：
      - `kmeansR.R`、`kmeansppR.R`、`spectralClusteringR.R`
      - 绘图函数：`plotKMeansResult.R`、`plotClusterCenters.R`、`plotSpectralClusteringResult.R`
    - **其他辅助函数**：
      - `data_docs.R`：数据文档相关函数。
      - `RcppExports.R`：Rcpp接口自动生成的导出函数。

- **`src/`**：
  - 存放C++源代码文件，通过Rcpp接口与R语言集成，实现了各类算法的高效计算版本：
    - `softmaxCpp.cpp`、`kmeansCpp.cpp`、`kmeansppCpp.cpp`、`naiveBayesCpp.cpp`、`spectralClustering.cpp`、`informationGainCpp.cpp`
    - 生成的目标文件和动态链接库：`*.o`、`SA24231055.dll`

- **`man/`**：
  - 包含所有函数的帮助文档文件（`.Rd`格式），如：
    - `softmaxR.Rd`、`softmaxCpp.Rd`、`decisionTreeR.Rd`
    - `kmeansR.Rd`、`kmeansCpp.Rd`、`kmeansppR.Rd`、`kmeansppCpp.Rd`
    - `naiveBayesR.Rd`、`naiveBayesCpp.Rd`
    - 绘图函数文档：`plotSoftmaxLoss.Rd`、`plotDecisionTreeAccuracy.Rd`等

- **`vignettes/`**：
  - 存放R包的教程与示例文档：
    - `intro.Rmd`：主要介绍R包的功能、使用方法及算法实现。

- **`inst/doc/`**：
  - 存放生成的文档文件：
    - `intro.html`：是对本R包使用的详细说明。
    - `homework.html`：汇总了所有平时的作业内容。

- **`DESCRIPTION`** 和 **`NAMESPACE`**：
  - 定义了R包的基本信息、依赖关系以及导出函数，确保包的正确加载与使用。

## 数据描述

本次期末R包设计所使用的数据集如下：

- **`films_train.rda`**：用于分类算法的训练数据集，共包含若干样本，每个样本有多维特征，最后一列为标签（整数值）。
- **`films_test.rda`**：用于分类算法的测试数据集，结构与 `films_train.rda` 相同。
- **`ori_data.rda`**：用于聚类算法的数据集，每个样本包含多维特征，没有标签信息。

### 数据集前五行展示

程序运行的最初，要加载包和忽略一些版本不兼容的警告。

```{r, eval=TRUE}
# 创建一个自定义的安全加载函数

safe_library <- function(pkg) {
  suppressPackageStartupMessages(
    suppressWarnings(
      library(pkg, character.only = TRUE)
    )
  )
}

# 加载需要的包
safe_library("Rcpp")
safe_library("microbenchmark")
safe_library("data.tree")
safe_library("DiagrammeR")
safe_library("matrixStats")
safe_library("knitr")
library(devtools)
load_all()
```

#### 1. 分类数据集（`films_train` 和 `films_test`）

首先，加载 `films_train` 和 `films_test` 数据集：

```{r, eval=TRUE}
library("SA24231055")  

# 加载数据集
data(films_train)
data(films_test)
```


**`films_train` 数据集前五行：**

```{r, eval=TRUE}
# 加载包
library(SA24231055)
head(films_train, 5)
```


**说明：**

- 数据集共有 7 列，其中前 6 列为特征，最后一列 `V7` 为标签列，取值为整数，表示类别。
- 样本的数据类型为整数，特征值范围在一定区间内。

**`films_test` 数据集前五行：**

```{r, eval=TRUE}
head(films_test, 5)
```

**说明：**

- 测试集的数据结构与训练集相同，可用于评估模型的泛化能力。

#### 2. 聚类数据集（`ori_data`）

加载 `ori_data` 数据集：

```{r, eval=TRUE}
# 加载数据
data(ori_data)
```

**`ori_data` 数据集前五行：**

```{r, eval=TRUE}
head(ori_data, 5)
```

**说明：**

- 数据集共有 16 列，所有列均为特征，没有标签列。
- 数据类型为数值型，包含整数和小数，特征值范围较广。

## 分类算法

本节将实现以下三种分类算法：

1. [Softmax 多分类算法](#softmax-多分类算法)
2. [决策树分类算法](#决策树分类算法)
3. [朴素贝叶斯分类算法](#朴素贝叶斯分类算法)

### Softmax 多分类算法

#### 解答思路

Softmax 多分类算法是一种常用的线性分类模型，适用于多类别分类问题。其核心思想是通过 Softmax 函数将线性模型的输出转换为概率分布，从而进行多类别预测。

**算法步骤：**

1. **模型初始化**：随机初始化权重矩阵 $\mathbf{W}$ 和偏置向量 $\mathbf{b}$。
2. **前向传播**：计算每个样本的线性输出和 Softmax 概率分布。
3. **损失计算**：使用交叉熵损失函数计算模型的损失。
4. **反向传播**：计算损失对权重和偏置的梯度。
5. **参数更新**：使用梯度下降算法更新参数。
6. **迭代训练**：重复步骤 2-5，直到达到指定的迭代次数或损失收敛。

#### 公式推导

**1. 线性输出：**

对于第 $i$ 个样本 $\mathbf{x}_i$，模型的线性输出为：

$$
\mathbf{z}_i = \mathbf{W}^\top \mathbf{x}_i + \mathbf{b}
$$

其中：

- $\mathbf{W} \in \mathbb{R}^{d \times K}$：权重矩阵，$d$ 为特征维度，$K$ 为类别数。
- $\mathbf{b} \in \mathbb{R}^K$：偏置向量。

**2. Softmax 函数：**

将线性输出转换为概率分布：

$$
P(y_i = j | \mathbf{x}_i) = \frac{e^{z_{ij}}}{\sum_{k=1}^K e^{z_{ik}}}
$$

其中：

- $z_{ij}$：第 $i$ 个样本在第 $j$ 个类别的得分。
- $P(y_i = j | \mathbf{x}_i)$：第 $i$ 个样本属于第 $j$ 类的概率。

**3. 损失函数（交叉熵损失）：**

$$
L = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^K y_{ij} \log P(y_i = j | \mathbf{x}_i)
$$

其中：

- $N$：样本总数。
- $y_{ij}$：指示函数，当第 $i$ 个样本的真实类别为 $j$ 时，$y_{ij} = 1$，否则为 $0$。

**4. 梯度计算：**

对权重矩阵 $\mathbf{W}$ 的梯度为：

$$
\frac{\partial L}{\partial \mathbf{W}} = \frac{1}{N} \sum_{i=1}^N \left( \mathbf{x}_i ( \hat{\mathbf{y}}_i - \mathbf{y}_i )^\top \right)
$$

其中：

- $\hat{\mathbf{y}}_i$：模型预测的概率分布向量。
- $\mathbf{y}_i$：真实的 one-hot 编码标签向量。

**5. 参数更新：**

使用学习率 $\eta$，参数更新为：

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{\partial L}{\partial \mathbf{W}}
$$

#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
library("ggplot2")
# 加载训练和测试数据
data(films_train)  # 确保已加载films_train数据集
data(films_test)   # 确保已加载films_test数据集


# 提取特征矩阵和标签向量
X_train <- as.matrix(films_train[, 1:(ncol(films_train) - 1)])
y_train <- films_train[, ncol(films_train)]

X_test <- as.matrix(films_test[, 1:(ncol(films_test) - 1)])
y_test <- films_test[, ncol(films_test)]

# 标签映射
unique_labels <- sort(unique(c(y_train, y_test)))
label_map <- setNames(0:(length(unique_labels) - 1), unique_labels)
y_train_mapped <- label_map[as.character(y_train)]
y_test_mapped <- label_map[as.character(y_test)]


# 调用softmaxR函数进行训练
model_softmax_R <- softmaxR(X_train, y_train_mapped, X_test, y_test_mapped, learning_rate=0.01, num_iters=1000)

# 获取损失历史数据框
loss_df <- model_softmax_R$loss_df

# 使用绘图函数绘制损失曲线
plotSoftmaxLoss(loss_df)

# 获取准确率历史数据框
accuracy_df <- model_softmax_R$accuracy_df

# 提取测试准确率的数据
test_accuracy <- accuracy_df$Accuracy[accuracy_df$Dataset == "测试准确率"]

# 获取最终的测试准确率
final_test_acc <- tail(test_accuracy, 1)

# 打印最终的测试准确率
cat(sprintf("最终的测试准确率: %.4f\n", final_test_acc))
```

**代码概述：**

- `softmaxR` 是用 R 语言实现的 Softmax 多分类算法函数。
- 将训练数据的特征和标签分别提取出来。
- 设置学习率为 `0.01`，迭代次数为 `100`。
- 绘制损失曲线，可以观察模型的收敛情况。

##### Rcpp 实现

```{r, eval=TRUE}
# 调用 Softmax Rcpp 版本函数进行训练
model_softmax_Cpp <- softmaxCpp(X_train, y_train, X_test, y_test, learning_rate = 0.01, epochs = 100)

# 创建一个数据框，包含迭代次数和对应的损失值
loss_df <- data.frame(
  Epoch = 1:length(model_softmax_Cpp$loss_history),
  Loss = model_softmax_Cpp$loss_history
)

# 使用 ggplot2 绘制损失曲线
ggplot(loss_df, aes(x = Epoch, y = Loss)) +
  geom_line(color = "#2c7fb8", linewidth = 1) +          # 绘制线条，设置颜色和线宽
  geom_point(color = "#2c7fb8", size = 0.5) +       # 添加数据点，设置颜色和大小
  geom_smooth(method = "loess", se = FALSE, color = "#fc8d59", linetype = "dashed") +  # 添加平滑曲线
  labs(
    title = "Softmax Rcpp 版本 - 损失曲线",
    x = "迭代次数",
    y = "损失值"
  ) +
  theme_minimal(base_size = 15) +                   # 使用简洁主题并设置基准字体大小
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", color = "black"),  # 设置标题样式
    axis.title = element_text(face = "bold", color = "black"),              # 设置轴标题样式
    panel.grid.major = element_line(color = "#e0e0e0"),                       # 设置主网格线颜色
    panel.grid.minor = element_blank()                                        # 移除次网格线
  )

print(paste("Softmax Rcpp版本在测试集上的准确率为", model_softmax_Cpp$test_accuracy))
```


**代码概述：**

- `softmaxCpp` 是用 Rcpp 实现的 Softmax 多分类算法函数。
- 使用 Rcpp 提高了计算效率，适合处理较大的数据集。
- 同样绘制损失曲线，观察模型的收敛情况。

#### 执行时间比较

为了比较 R 和 Rcpp 实现的 Softmax 多分类算法的执行效率，使用 `microbenchmark` 包对两种实现方式进行多次运行，记录其执行时间，并将结果进行对比。

```{r, eval=TRUE}
# 设置测试参数
learning_rate <- 0.01
epochs <- 1000

# 定义测试函数
benchmark_softmax <- microbenchmark(
  Softmax_R = {
    model_softmax_R <- softmaxR(X_train, y_train, X_test, y_test, learning_rate, epochs)
  },
  Softmax_Rcpp = {
    model_softmax_Cpp <- softmaxCpp(X_train, y_train, X_test, y_test, learning_rate, epochs)
  },
  times = 10
)

# 查看执行时间结果
print(benchmark_softmax)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_softmax)

# 计算平均执行时间（毫秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_ms <- mean_times$time / 1e6  # 纳秒转换为毫秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_ms")], caption = "Softmax 多分类算法执行时间比较")

```

**表格分析：**

从上表可以看出，Rcpp 实现的 Softmax 多分类算法的平均执行时间明显短于 R 语言实现。这表明使用 Rcpp 能够显著提高计算效率，尤其是在迭代次数较多或数据量较大的情况下，Rcpp 的优势更加明显。

#### 图形展示

```{r, eval=TRUE}
# 获取softmaxR的准确率历史数据框
accuracy_df <- model_softmax_R$accuracy_df

# 使用绘图函数绘制准确率曲线
plotSoftmaxAccuracy(accuracy_df)
```


**代码概述：**

- `plotSoftmaxAccuracy` 函数用于绘制模型在训练集和测试集上的准确率变化曲线。
- 可以直观地观察softmaxR模型的性能随迭代次数的变化。

#### 结果分析

- **模型收敛性**：从损失曲线可以看出，随着迭代次数的增加，模型的损失逐渐降低，表明模型在逐步收敛。
- **准确率变化**：准确率曲线显示，训练集和测试集的准确率都在逐步提高，最终趋于稳定。
- **R 与 Rcpp 比较**：Rcpp 实现的版本在计算速度上明显优于 R 版本，适合处理更大的数据集。

### 决策树分类算法

#### 解答思路

决策树是一种树形结构的分类模型，通过对数据集进行递归划分来构建分类规则。本次实现的决策树算法使用信息增益作为特征选择的标准，树的最大深度限定为 5，以防止过拟合。

**算法步骤：**

1. **计算信息熵**：计算当前数据集的熵，衡量数据集的纯度。
2. **选择最优特征**：计算每个特征的信息增益，选择信息增益最大的特征进行划分。
3. **递归构建子树**：对划分后的子集，重复步骤 1 和 2，直到满足停止条件（如达到最大深度或节点纯度为 1）。
4. **构建叶子节点**：当无法继续划分时，确定叶子节点的类别。

#### 公式推导

**1. 信息熵：**

$$
H(D) = -\sum_{k=1}^K p_k \log_2 p_k
$$

其中：

- $D$：当前数据集。
- $K$：类别数。
- $p_k$：数据集中属于第 $k$ 类的样本比例。

**2. 信息增益：**

$$
Gain(D, A) = H(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} H(D^v)
$$

其中：

- $A$：特征。
- $V$：特征 $A$ 的可能取值数目。
- $D^v$：在特征 $A$ 上取值为 $v$ 的子集。

#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
# 加载数据
data(films_train)
data(films_test)

# 使用决策树算法训练模型
tree <- decisionTreeR(films_train, max_depth = 5)

# 定义预测函数
predict_tree <- function(tree, sample) {
  if (tree$type == "leaf") {
    return(tree$class)
  } else {
    feature_value <- sample[[tree$feature]]
    branch <- tree$branches[[as.character(feature_value)]]
    if (is.null(branch)) {
      # 若没有对应的分支，返回当前节点中样本最多的类别
      return(tree$class)
    } else {
      return(predict_tree(branch, sample))
    }
  }
}

# 在测试集上进行预测
predictions <- apply(films_test, 1, function(row) predict_tree(tree, as.list(row)))

# 计算准确率
accuracy <- mean(predictions == films_test[, ncol(films_test)])
print(paste("R语言实现的决策树算法在测试集上的准确率：", round(accuracy * 100, 2), "%"))
```

**代码概述：**

- `decisionTreeR` 函数实现了决策树的训练过程。
- `predict_tree` 函数用于对单个样本进行预测。
- 计算测试集的准确率，评估模型性能。

##### Rcpp 实现信息增益计算

```{r, eval=TRUE}
# 计算某个特征的信息增益（例如第二个特征）
gain <- informationGainCpp(films_train, "V2")
print(paste("特征 V2 的信息增益：", gain))
```

**代码概述：**

- `informationGainCpp` 函数使用 Rcpp 实现了信息增益的计算，提高了计算效率。
- 可以将其集成到决策树的构建过程中。

#### 执行时间比较

为了比较 R 和 Rcpp 实现的决策树算法在信息增益计算部分的执行效率，我对信息增益计算进行多次运行，记录其执行时间。

```{r, eval=TRUE}
# 加载 microbenchmark 包
library(microbenchmark)

# 定义测试函数
benchmark_info_gain <- microbenchmark(
  InfoGain_R = {
    gain_R <- information_gain(films_train, "V2")
  },
  InfoGain_Rcpp = {
    gain_Cpp <- informationGainCpp(films_train, "V2")
  },
  times = 100
)

# 查看执行时间结果
print(benchmark_info_gain)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_info_gain)

# 计算平均执行时间（微秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_us <- mean_times$time / 1e3  # 纳秒转换为微秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_us")], caption = "信息增益计算执行时间比较")
```


**表格分析：**

从上表可以看出，Rcpp 实现的信息增益计算平均执行时间明显短于 R 语言实现。这在构建决策树时，需要多次计算信息增益，使用 Rcpp 可以显著提高模型训练的效率。

#### 图形展示

```{r, eval=TRUE}
# 绘制决策树的准确率对比图
plotDecisionTreeAccuracy(tree, films_train, films_test)
```

```{r, eval=TRUE}
# 绘制决策树的结构图
plotDecisionTreeStructure(tree)
```

#### 结果分析

- **准确率表现**：决策树在训练集和测试集上都取得了较高的准确率，表明模型具有良好的泛化能力。
- **模型可解释性**：通过决策树的结构图，可以直观地理解模型的决策过程，有助于发现特征与标签之间的关系。
- **深度限制的影响**：将树的深度限制为 5，有助于防止模型过拟合，提高模型的泛化能力。

### 朴素贝叶斯分类算法

#### 解答思路

朴素贝叶斯算法是一种基于贝叶斯定理和特征条件独立假设的简单而有效的分类方法。其核心思想是通过计算样本在各个类别下的后验概率，选择概率最大的类别作为预测结果。

**算法步骤：**

1. **先验概率计算**：计算每个类别的先验概率 $P(C_k)$。
2. **条件概率计算**：计算在类别 $C_k$ 下，各特征取值的条件概率 $P(X_i | C_k)$。
3. **分类预测**：对于新的样本，计算各类别的后验概率 $P(C_k | X)$，选择概率最大的类别。

#### 公式推导

**1. 贝叶斯定理：**

$$
P(C_k | X) = \frac{P(C_k) \prod_{i=1}^n P(X_i | C_k)}{P(X)}
$$

其中：

- $C_k$：第 $k$ 个类别。
- $X = (X_1, X_2, ..., X_n)$：特征向量。
- $P(C_k)$：类别先验概率。
- $P(X_i | C_k)$：在类别 $C_k$ 下特征 $X_i$ 的条件概率。
- $P(X)$：特征向量的边缘概率，因对所有类别相同，可忽略。

**2. 特征条件独立假设：**

假设特征之间相互独立，即：

$$
P(X | C_k) = \prod_{i=1}^n P(X_i | C_k)
$$

#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
# 加载数据
data(films_train)
data(films_test)

# 训练朴素贝叶斯模型
model_nb_R <- naiveBayesR(films_train)
```

**代码概述：**

- `naiveBayesR` 函数实现了朴素贝叶斯模型的训练过程。
- `predict_nb` 函数用于对新样本进行预测，计算各类别的后验概率。
- 通过在测试集上计算准确率，评估模型性能。

##### Rcpp 实现

```{r, eval=TRUE}
# 训练朴素贝叶斯模型（Rcpp 版本）
model_nb_Cpp <- naiveBayesCpp(as.matrix(films_train))
```

#### 执行时间比较

对朴素贝叶斯算法的训练过程进行执行时间比较。

```{r, eval=TRUE}
# 加载 microbenchmark 包
library(microbenchmark)

# 定义测试函数
benchmark_nb <- microbenchmark(
  NaiveBayes_R = {
    model_nb_R <- naiveBayesR(films_train)
  },
  NaiveBayes_Rcpp = {
    model_nb_Cpp <- naiveBayesCpp(as.matrix(films_train))
  },
  times = 100
)

# 查看执行时间结果
print(benchmark_nb)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_nb)

# 计算平均执行时间（微秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_us <- mean_times$time / 1e3  # 纳秒转换为微秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_us")], caption = "朴素贝叶斯训练执行时间比较")
```

**表格分析：**

从上表可以看出，Rcpp 实现的朴素贝叶斯算法训练速度快于 R 语言实现。对于需要频繁更新模型的场景，Rcpp 的优势更加明显。

#### 图形展示

```{r, eval=TRUE}
# 绘制朴素贝叶斯分类器的准确率对比图
plotNaiveBayesAccuracy(model_nb_R, films_train, films_test)
```

#### 结果分析

- **模型性能**：朴素贝叶斯在训练集和测试集上都取得了较高的准确率，证明了其在特征条件独立假设下的有效性。
- **计算效率**：朴素贝叶斯模型计算简单，训练和预测速度快，适合处理大规模数据集。

## 聚类算法

本部分将实现以下三种聚类算法：

1. [K-Means 算法](#k-means-算法)
2. [K-Means++ 算法](#k-means-算法)
3. [谱聚类算法](#谱聚类算法)

### K-Means 算法

#### 解答思路

K-Means 是一种常用的聚类算法，其目标是将数据集划分为 $k$ 个簇，使得同一簇内的样本尽可能相似，不同簇之间的样本尽可能不同。

**算法步骤：**

1. **初始化**：随机选择 $k$ 个样本作为初始聚类中心。
2. **分配样本**：将每个样本分配到距离最近的聚类中心所属的簇。
3. **更新聚类中心**：计算每个簇的样本均值，作为新的聚类中心。
4. **迭代**：重复步骤 2 和 3，直到聚类中心不再发生变化或达到最大迭代次数。

#### 公式推导

**目标函数（优化目标）：**

$$
J = \sum_{i=1}^k \sum_{\mathbf{x}_j \in C_i} || \mathbf{x}_j - \boldsymbol{\mu}_i ||^2
$$

其中：

- $C_i$：第 $i$ 个簇。
- $\boldsymbol{\mu}_i$：第 $i$ 个簇的中心（均值向量）。
- $|| \cdot ||$：欧氏距离。

#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
# 加载数据
data(ori_data)

# 检查数据类型
class(ori_data)  # 查看数据类型

# 将数据转换为矩阵
ori_data_matrix <- as.matrix(ori_data)

# 使用 K-Means 算法进行聚类
result_kmeans_R <- kmeansR(ori_data_matrix, k = 3)

# 查看聚类结果
table(result_kmeans_R$clusters)

# 绘制聚类中心热图
plotClusterCenters(result_kmeans_R$centers)
```

**代码概述：**

- `kmeansR` 函数实现了 K-Means 聚类算法。
- 选择将数据分为 3 个簇。
- 可以通过聚类结果的分布，了解各簇的样本数量。

##### Rcpp 实现

```{r, eval=TRUE}
# 使用 K-Means Rcpp 版本进行聚类
result_kmeans_Cpp <- kmeansCpp(as.matrix(ori_data), k = 3)
```

#### 执行时间比较

对 K-Means 算法的聚类过程进行执行时间比较。

```{r, eval=TRUE}
# 加载 microbenchmark 包
library(microbenchmark)

# 定义测试函数
benchmark_kmeans <- microbenchmark(
  KMeans_R = {
    result_kmeans_R <- kmeansR(as.matrix(ori_data), k = 3)
  },
  KMeans_Rcpp = {
    result_kmeans_Cpp <- kmeansCpp(as.matrix(ori_data), k = 3)
  },
  times = 10
)

# 查看执行时间结果
print(benchmark_kmeans)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_kmeans)

# 计算平均执行时间（毫秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_ms <- mean_times$time / 1e6  # 纳秒转换为毫秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_ms")], caption = "K-Means 算法执行时间比较")
```

**表格分析：**

Rcpp 实现的 K-Means 算法在执行时间上比 R 语言实现更快，尤其在数据量较大的情况下，Rcpp 能够有效地提升计算效率。

#### 图形展示

```{r, eval=TRUE}
# 绘制 K-Means 聚类结果
plotKMeansResult(as.matrix(ori_data), result_kmeans_R$clusters)
```

#### 结果分析

- **聚类效果**：通过可视化，可以观察到样本在二维空间中的分布，评估聚类的合理性。
- **初始中心的影响**：由于初始中心的随机性，可能导致聚类结果的不稳定。

### K-Means++ 算法

#### 解答思路

K-Means++ 算法改进了 K-Means 算法的初始中心选择策略，旨在提高聚类效果和收敛速度。

**算法步骤：**

1. **第一个中心的选择**：随机选择一个样本作为第一个聚类中心。
2. **后续中心的选择**：对于每个样本，计算其与最近已有中心的距离平方，按照距离平方的概率分布选择下一个中心。
3. **重复步骤 2**：直到选择了 $k$ 个聚类中心。
4. **后续步骤**：与 K-Means 算法相同。

#### 公式推导

**概率分布：**

对于每个样本 $\mathbf{x}$，选择它作为下一个中心的概率为：

$$
P(\mathbf{x}) = \frac{D(\mathbf{x})^2}{\sum_{\mathbf{x} \in X} D(\mathbf{x})^2}
$$

其中，$D(\mathbf{x})$ 是样本 $\mathbf{x}$ 与最近已选中心的距离。

#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
# 使用 K-Means++ 算法进行聚类
result_kmeanspp_R <- kmeansppR(as.matrix(ori_data), k = 5)

# 查看聚类结果
table(result_kmeanspp_R$clusters)

# 绘制聚类中心热图
plotClusterCenters(result_kmeanspp_R$centers)
```

**代码概述：**

- `kmeansppR` 函数实现了 K-Means++ 聚类算法。
- K-Means++ 通常能取得比 K-Means 更稳定的聚类结果。

##### Rcpp 实现

```{r, eval=TRUE}
# 使用 K-Means++ Rcpp 版本进行聚类
result_kmeanspp_Cpp <- kmeansppCpp(as.matrix(ori_data), k = 5)
```

#### 执行时间比较

对 K-Means++ 算法的聚类过程进行执行时间比较。

```{r, eval=TRUE}
# 定义测试函数
benchmark_kmeanspp <- microbenchmark(
  KMeanspp_R = {
    result_kmeanspp_R <- kmeansppR(as.matrix(ori_data), k = 5)
  },
  KMeanspp_Rcpp = {
    result_kmeanspp_Cpp <- kmeansppCpp(as.matrix(ori_data), k = 5)
  },
  times = 10
)

# 查看执行时间结果
print(benchmark_kmeanspp)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_kmeanspp)

# 计算平均执行时间（毫秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_ms <- mean_times$time / 1e6  # 纳秒转换为毫秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_ms")], caption = "K-Means++ 算法执行时间比较")
```

**表格分析：**

与 K-Means 算法类似，Rcpp 实现的 K-Means++ 算法在执行时间上更具优势，提高了算法的效率。

#### 图形展示

```{r, eval=TRUE}
# 绘制 K-Means++ 聚类结果
plotKMeansResult(as.matrix(ori_data), result_kmeanspp_R$clusters)
```

#### 结果分析

- **聚类效果提升**：与 K-Means 相比，K-Means++ 由于更好的初始中心选择，通常能取得更好的聚类效果。
- **收敛速度**：K-Means++ 通常能更快地收敛，减少迭代次数。

#### 解答思路

谱聚类是一种基于图论和线性代数的聚类算法，适用于处理复杂形状和结构的数据。其核心思想是通过构建样本之间的相似度矩阵，计算拉普拉斯矩阵，并对其进行特征值分解，将数据映射到低维空间。在低维空间中，数据的聚类结构更加明显，从而可以使用传统的聚类算法（如K-Means）进行有效的聚类。

### 谱聚类算法

#### 算法步骤

1. **数据归一化**：

   - 将数据矩阵 `X` 中的每个特征缩放到 `[0, 1]` 的范围内。这样做的目的是确保不同特征在相似度计算中具有相同的权重，避免某些特征因尺度较大而主导聚类结果。

2. **构建相似度矩阵 W**：

   - 计算样本之间的相似度，通常使用高斯核函数（Gaussian Kernel）来衡量样本间的相似度。

   - 公式为：

$$
     W_{ij} = \exp\left( -||\mathbf{x}_i - \mathbf{x}_j||^2 \right)
$$
     
  其中，$\mathbf{x}_i$ 和 $\mathbf{x}_j$ 分别表示第 $i$ 个和第 $j$ 个样本，$||\cdot||$ 表示欧氏距离。

3. **构建拉普拉斯矩阵 L**：

   - 计算度矩阵 $D$，其对角线元素 $D_{ii}$ 为相似度矩阵 $W$ 的第 $i$ 行元素之和：

$$
     D_{ii} = \sum_{j} W_{ij}
$$

   - 构建拉普拉斯矩阵：
     
$$
     L = D - W
$$

4. **特征值分解**：

   - 对拉普拉斯矩阵 $L$ 进行特征值分解，得到特征值和对应的特征向量。
   - 在 R 语言中，`eigen` 函数返回的特征值是按降序排列的，因此选择最后 $k$ 个特征值对应的特征向量。

5. **选择特征向量**：

   - 取最小的 $k$ 个特征值对应的特征向量，构成矩阵 $U$。每一行对应一个样本在新特征空间中的表示。

6. **定义聚类中心**：

   - 将矩阵 $U$ 的前 $k$ 行作为聚类中心。这意味着每个聚类中心是一个 $k$ 维向量，对应于特征空间中的一个点。

7. **聚类分配**：

   - 对于每个样本，计算其在特征空间中与每个聚类中心的欧氏距离，并将其分配到距离最近的簇。
   - 具体步骤如下：
     - 计算样本与每个聚类中心的距离。
     - 将样本分配到距离最小的簇。

#### 公式推导

1. **高斯核函数**：

   - 用于衡量样本之间的相似度，公式如下：

$$
     W_{ij} = \exp\left( -||\mathbf{x}_i - \mathbf{x}_j||^2 \right)
$$
     其中，$W_{ij}$ 表示样本 $i$ 和样本 $j$ 之间的相似度，$\mathbf{x}_i$ 和 $\mathbf{x}_j$ 分别是第 $i$ 个和第 $j$ 个样本。

2. **度矩阵**：

   - 度矩阵 $D$ 是一个对角矩阵，其对角线元素表示每个节点（样本）的度数，即与之相连的边的权重之和：
     
$$
     D_{ii} = \sum_{j} W_{ij}
$$

3. **拉普拉斯矩阵**：

   - 拉普拉斯矩阵 $L$ 定义为度矩阵减去相似度矩阵：

$$
     L = D - W
$$

4. **特征值分解**：

   - 对拉普拉斯矩阵进行特征值分解，得到特征值 $\lambda$ 和对应的特征向量 $\mathbf{u}$：

$$
     L \mathbf{u} = \lambda \mathbf{u}
$$

5. **特征向量选择**：

   - 选择对应最小的 $k$ 个特征值的特征向量，构成矩阵 $U$。由于在 R 中，`eigen` 函数按降序排列特征值，因此选择最后 $k$ 个特征向量。

6. **聚类中心定义**：

   - 将矩阵 $U$ 的前 $k$ 行作为聚类中心，每个聚类中心是一个 $k$ 维向量：

$$
     \text{centroid}_j = \mathbf{u}_j, \quad j = 1, 2, \ldots, k
$$

7. **聚类分配公式**：

   - 对于每个样本 $i$，计算其与每个聚类中心 $j$ 的距离，并分配到最近的簇：
     
$$
     \text{label}_i = \arg\min_{j} \left( ||\text{centroid}_j - \mathbf{u}_i||^2 \right)
$$



#### 代码实现

##### R 语言实现

```{r, eval=TRUE}
# 使用谱聚类算法进行聚类
clusters_spectral_R <- spectralClusteringR(as.matrix(ori_data), k = 3)

# 查看聚类结果
table(clusters_spectral_R)
```

**代码概述：**

- `spectralClusteringR` 函数实现了谱聚类算法。
- 参数 `sigma` 控制高斯核函数的带宽，对相似度计算有重要影响。

##### Rcpp 实现

```{r, eval=TRUE}
# 应用谱聚类算法
clusters_spectral_cpp <- spectralClusteringRcpp(as.matrix(ori_data), 3)

# 查看聚类结果
print(table(clusters_spectral_cpp))
```

#### 执行时间比较

对谱聚类算法R语言实现和Rcpp实现的执行时间进行比较。

```{r, eval=TRUE}
library(microbenchmark)
# 进行性能比较
benchmark_result <- microbenchmark(
  R_version = spectralClusteringR(as.matrix(ori_data), 3),
  Rcpp_version = spectralClusteringRcpp(as.matrix(ori_data), 3),
  times = 3  # 可以根据需要调整重复次数
)

# 查看结果
print(benchmark_result)

# 将结果转换为数据框
benchmark_df <- as.data.frame(benchmark_result)

# 计算平均执行时间（纳秒转为秒）
mean_times <- aggregate(time ~ expr, data = benchmark_df, FUN = mean)
mean_times$time_s <- mean_times$time / 1e9  # 纳秒转换为秒

# 显示平均执行时间
knitr::kable(mean_times[, c("expr", "time_s")], caption = "谱聚类算法 R 版本与 Rcpp 版本的平均执行时间 (秒)")

# 可视化比较结果
ggplot(benchmark_df, aes(x = expr, y = time)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  ggtitle("谱聚类算法 R 版本与 Rcpp 版本执行时间比较") +
  ylab("时间 (纳秒)") +
  xlab("方法") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

**表格分析：**

相似度矩阵计算涉及大量的矩阵运算，使用 Rcpp 能够大幅提升计算速度，在谱聚类算法中，这对于处理较大的数据集非常重要。

#### 图形展示

```{r, eval=TRUE}
# 绘制谱聚类结果
plotSpectralClusteringResult(as.matrix(ori_data), clusters_spectral_R)
```

#### 结果分析

- **捕捉非线性结构**：谱聚类能够有效地处理具有复杂结构的数据，捕捉到传统聚类算法难以发现的模式。
- **参数敏感性**：相似度矩阵的构建对参数（如 $\sigma$）较为敏感，需要通过实验调整。

## 各算法对比与分析

### 分类算法对比

- **Softmax 多分类算法**：
  - 优点：适用于多类别分类问题，模型简单，易于实现。
  - 缺点：对线性可分的数据表现较好，对于非线性数据可能效果不佳。
  - **R 与 Rcpp 比较**：Rcpp 实现的版本在计算效率上更有优势，适合大规模数据。

- **决策树分类算法**：
  - 优点：模型可解释性强，能处理非线性和高维数据。
  - 缺点：容易过拟合，需要进行剪枝或限制树深度。
  - **深度限制**：通过限制树的最大深度，可以控制模型复杂度，防止过拟合。

- **朴素贝叶斯分类算法**：
  - 优点：算法简单，计算速度快，适合高维数据。
  - 缺点：特征条件独立假设在实际中往往不成立，可能影响模型性能。

### 聚类算法对比

- **K-Means 算法**：
  - 优点：算法简单，易于实现，计算速度快。
  - 缺点：对初始中心敏感，可能陷入局部最优；只能发现凸形簇。
  - **改进空间**：使用 K-Means++ 或多次运行取最优结果。

- **K-Means++ 算法**：
  - 优点：改进了初始中心的选择，提高了聚类效果和收敛速度。
  - 缺点：初始中心的随机性仍然存在，但影响较小。

- **谱聚类算法**：
  - 优点：能够处理非凸形状的簇，适用于复杂结构的数据。
  - 缺点：计算复杂度较高，适合中小规模数据；相似度矩阵的构建对参数敏感。

## 总结与展望

通过本次实验，我深入理解并实现了多种经典的分类和聚类算法，掌握了从算法原理到代码实现的全过程。同时，利用 R 和 Rcpp 提高了算法的执行效率，并通过丰富的图形展示了算法的效果。

在实际应用中，选择合适的算法需要根据数据的特点和任务的需求。未来可以进一步探索以下方向：

- **算法优化**：针对大规模数据，进一步优化算法的效率，如并行计算等。
- **模型融合**：结合多种算法的优势，构建更强大的模型。
- **参数调优**：通过交叉验证等方法，自动选择最优的模型参数。

---

